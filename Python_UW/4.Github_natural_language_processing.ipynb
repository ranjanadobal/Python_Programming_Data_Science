{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing - Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rYYTxll6wm6i"
   },
   "source": [
    "Ref: https://www.tiesdekok.com\n",
    "\n",
    "**Python version:** Python 3.6+     \n",
    "**Recommended environment: `researchPython`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "recommendedEnvironment = 'researchPython'\n",
    "if os.environ['CONDA_DEFAULT_ENV'] != recommendedEnvironment:\n",
    "    print('Warning: it does not appear you are using the {0} environment, did you run \"conda activate {0}\" before starting Jupyter?'.format(recommendedEnvironment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevant tutorial notebooks:\n",
    "\n",
    "1) [`0_python_basics.ipynb`](https://nbviewer.jupyter.org/github/TiesdeKok/LearnPythonforResearch/blob/master/0_python_basics.ipynb)  \n",
    "\n",
    "\n",
    "2) [`2_handling_data.ipynb`](https://nbviewer.jupyter.org/github/TiesdeKok/LearnPythonforResearch/blob/master/2_handling_data.ipynb)  \n",
    "\n",
    "\n",
    "3) [`NLP_Notebook.ipynb`](https://nbviewer.jupyter.org/github/TiesdeKok/Python_NLP_Tutorial/blob/master/NLP_Notebook.ipynb)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (3.4.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from spacy) (1.23.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from spacy) (1.10.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from spacy) (1.0.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from spacy) (0.8.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: setuptools in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from spacy) (65.5.1)\n",
      "Requirement already satisfied: jinja2 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from spacy) (0.4.2)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from spacy) (8.1.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from spacy) (2.4.5)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from jinja2->spacy) (2.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-lg==3.4.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.4.1/en_core_web_lg-3.4.1-py3-none-any.whl (587.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m759.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from en-core-web-lg==3.4.1) (3.4.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.8)\n",
      "Requirement already satisfied: setuptools in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (65.5.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (4.64.1)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.4.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (21.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.10.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.0.8)\n",
      "Requirement already satisfied: jinja2 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.1.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.0.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.28.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.4.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.10)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.8.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (8.1.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.0.9)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.0.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Applications/anaconda3/envs/researchPython/lib/python3.9/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have to replace the above with the code below if you installed the language model in an alternative way\n",
    "```python\n",
    "import en_core_web_lg\n",
    "nlp = en_core_web_lg.load()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Perform basic operations on a sample earnings transcript text file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a) Load the following text file: `data > example_transcript.txt` into Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MDA_files',\n",
       " 'LoughranMcDonald_MasterDictionary_2014.xlsx',\n",
       " 'MDA_META_DF.xlsx',\n",
       " 'example_transcript.txt',\n",
       " '.ipynb_checkpoints']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('data','example_transcript.txt'), 'r', encoding = 'utf-8') as f:\n",
    "    transcript = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ufeffCompany Name: Hope Bancorp Inc\\nCompany Ticker: HOPE US Equity\\nDate: 2020-01-23\\nQ4 2019 Earnings Call\\nCompany Participants\\nAlex Ko, Executive Vice President and Chief Financial Officer\\nAngie Yang, Director, Investor Relations\\nKevin S. Kim, President and Chief Executive Officer\\nOther Participants\\nBob Sean, Analyst\\nChristopher McGratty, Analyst\\nJake Stern, Analyst\\nTim Coffey, Analyst\\nUnidentified Participant\\nPresentation\\nOperator\\nHello and welcome to the Hope Bancorp Q4 2019 Earnings Conference Call. All participants will be in listen-only mode. (Operator Instructions) Please note this event is being recorded.\\n\\nI would now like to turn the conference over to Angie Yang, Director of Investor Relations. Please go ahead.\\n\\nAngie Yang \\nThank you, Keith. Good morning everyone and thank you for joining us for the Hope Bancorp 2019 Fourth Quarter Investor Conference Call. As usual we will begin -- we will be using a slide presentation to accompany our discussion this morning. If you have not done so already, please visit the Presentations page of our Investor Relations website to download a copy of the presentation. Or if you are listening into the webcast, you should be able to view the slides from your computer screen as we progress through the presentation.\\n\\nBeginning on Slide 2. I'd like to begin with a brief statement regarding forward-looking remarks. The call today may contain forward-looking projections regarding the future financial performance of the Company and future events. These statements are based on current expectations, estimates, forecasts, projections and management assumptions about the future performance of the Company as well as the businesses and markets in which the Company does and is expected to operate. These statements constitute forward-looking statements within the meaning of the US Private Securities Litigation Reform Act of 1995. These statements are not guarantees of future performance. Actual outcomes and results may differ materially from what is expressed or forecasted in such forward-looking statements.\\n\\nWe refer you to the documents the Company files periodically with the SEC as well as the Safe Harbor statements in our press release issued yesterday. Hope Bancorp assumes no obligation to revise any forward-looking projections that may be made on today's call. The Company cautions that the complete financial results to be included in the quarterly report on Form 10-K for the year ended December 31, 2019 could differ materially from the financial results being reported today.\\n\\nIn addition, some of the information referenced on this call today are non-GAAP financial measures. Please refer to our 2019 fourth quarter earnings release for the reconciliation of GAAP to non-GAAP financial measures.\\n\\nNow, we have allotted one hour for this call. Presenting from the management side today will be Kevin Kim, Hope Bancorp's Chairman, President and CEO and Alex Ko, our Executive Vice President and Chief Financial Officer. Chief Credit Officer, Peter Koh is also here with us today and will participate in the Q&A session.\\n\\nWith that, let me turn the call over to Kevin Kim. Kevin?\\n\\nKevin S. Kim \\nThank you, Angie. Good morning everyone and thank you for joining us today. Let's begin with Slide 3. Our fourth quarter results capped a successful year of consistent execution on our strategic priorities for enhancing the value of the Bank of Hope franchise. We continue to deliver a high level of profitability generating $43 million in net income or $0.34 per diluted share in the fourth quarter of 2019. This reflects a slight improvement over the $42.6 million or $0.34 in the preceding third quarter. Taking a step back and looking at the full year, you may recall on our fourth quarter earnings call last January, we discussed a number of key priorities for the year.\\n\\nI'm very pleased to report that we have made excellent progress on these initiatives. First and most notably, over the course of 2019, we dramatically improved our core deposits, particularly at the branch level where we are having a great deal of success in attracting retail depositors to our lower cost deposit accounts.\\n\\nDuring 2019, we increased our money market and now deposit balances by 31%, our savings deposits by 21% and our non-interest bearing deposits by 2%. This strong growth in core deposits enabled us to significantly reduce our dependence on higher costing time deposits.\\n\\nAs a result, at the end of 2019, time deposits have declined to 41.2% of our total deposits, down from 48.3% at the end of the prior year. The success we had in executing on our core deposit gathering initiatives proved critical in helping us to better manage our deposit costs, which plateaued during the third quarter of 2019 and then declined significantly during the fourth quarter.\\n\\nSecond, another key priority was tightly managing our non-interest expense levels. Through a consistent focus on enhancing efficiencies throughout our organization, we were able to reduce our non-interest expense as a percentage of average assets to 1.86% for the year, down from 1.88% in 2018. We were able to achieve this improvement despite the ongoing investments and continued spending on projects, including the implementation of CECL. And third, which is always a priority for Hope Bancorp was effective capital management. Through our stock repurchase program and attractive dividend, we returned $84.7 million of capital to shareholders in 2019, while maintaining strong capital ratios to support future growth.\\n\\nMoving on to Slide 4, in addition to the success we have had in gathering lower cost deposits, we were able to be more active in our loan production. As a result, we generated a record level of loan originations in the fourth quarter. We had $848 million in new loan originations funded in the fourth quarter, up from $694 million in the preceding third quarter.\\n\\nWe also saw a record level of payoffs, which totaled $486 million in the fourth quarter, up from $461 million in the prior quarter. Aggregate payoffs and paydowns totaled $668 million in the fourth quarter versus $633 million in the preceding quarter. We continued to see the larger mainstream banks targeting smaller lending opportunities than they have in the past and offering very aggressive pricing to win deals. However, with our record level of loan originations, our total loans increased $171 million or 1.4% from the end of the preceding quarter. This equates to 5.7% on an annualized basis.\\n\\nLooking at the breakdown of our loan production by major category, we continue to have a well-balanced mix of new loan originations. Commercial real estate loans comprised 61% of total production in the quarter. Commercial loans accounted for 31% and consumer loans, comprised primarily of residential mortgage loans, accounted for 8%.\\n\\nWe originated $513 million in CRE loans for the quarter, up from $349 million in the preceding third quarter. With the lower interest rate, we saw a modest increase in the demand for CRE loans and we capitalized on some attractive opportunities in a number of areas including hospitality, mixed use facilities and warehouses. We had another strong quarter of C&I loan originations. We funded $266 million in new C&I production in the fourth quarter. In addition to the expansion of our corporate banking group targeting lending opportunities with middle market mainstream entities, we have also been focused on building on our relationships with existing seasoned operators of top tier franchises, which has been contributing to our C&I loan production.\\n\\nTurning to our SBA business. We originated $62 million in SBA loans which reflects an increase from $55 million in the preceding third quarter. And since we are retaining this production, which is booked at higher rates than our overall average loans, we continue to see the positive impact this strategy is having on our average loan yields.\\n\\nWith that as an overview of our business development efforts, I will ask Alex to provide additional details on our financial performance for the fourth quarter. Alex?\\n\\nAlex Ko \\nThank you, Kevin. As I review our financial results, I will limit my discussion to just some of the more significant items in the quarter. Beginning with Slide 5. I will start with our net interest income which totaled $113.5 million compared with $116.3 million in the preceding third quarter. This decrease was mainly due to lower interest income on loans due to the impact of the interest rate cuts in September and October.\\n\\nOur net interest margin declined by 9 basis points to 3.16%. On a core basis, excluding purchase accounting adjustment, our net interest margin declined by 10 basis points. This was relatively in line with our expectations given the impact of both September and October rate cuts on our earnings asset yield.\\n\\nOur core loan yield, which exclude accretion income, declined 24 basis points from the preceding third quarter. This was driven by the repricing on our variable-rate loans following the September and October rate cuts, the payoffs of higher yielding loans and lower rate on new originations in the lower interest rate environment. This was partially offset by a substantial decline in our cost of deposits due to the mix -- funding mix that Kevin discussed. Our cost of deposits declined 13 basis point from the prior quarter to 1.49% and I would note that this is the first quarter-over-quarter decrease in deposit costs in three years. The last time deposit cost declined was in the fourth quarter of 2016 when the total cost of deposits decreased by 1 basis point. As we guided last quarter, our total cost of deposits has continued its downward trend since the peak in July 2019.\\n\\nOn a month-by-month basis, our cost of the deposits was 1.54% in October, 1.49% in November and 1.44% in December 2019. With a full quarter impact of October rate cuts and assuming no further changes in the Fed funds rate, we expect to see a decline in our net interest margin, excluding accretion income of approximately 2 to 3 basis points in the first quarter of 2020.\\n\\nNow moving on to Slide 6. Our non-interest income was $13 million, essentially unchanged from the preceding third quarter. The primary variance from the preceding quarter was a $1.1 million increase in the net gain on sale of other loans. During the quarter, total sales of mortgage loans amounted to $32.5 million compared with $30.9 million in the preceding quarter.\\n\\nIn addition, we completed note sale of an aggregate $26 million of CRE loans, which had been transferred to loans held for sale at the end of 2019 third quarter. This increase in gain on sale of other loans was partially offset by a $900,000 decline in swap fee income quarter-over-quarter, which reduced our other income and fees.\\n\\nMoving on to non-interest expenses on Slide 7. Our non-interest expense was $70.4 million, up modestly from the preceding third quarter. In terms of significant variances, our salaries and employee benefit expenses declined by $1.8 million due to lower levels of retirement benefit expense as well as lower self funded group insurance cost, which fluctuates each quarter based on the volume of insurance claims in a given quarter. These declines were offset by an increase in our FDIC assessment fees for which we reported no expense in the third quarter of 2019. We utilized a remainder of our small bank assessment and credit to offset some, but not all of our assessment fee expenses this quarter. And now, we also had an increase in credit-related expenses and a lower amount of OREO-related income driven by fluctuation in OREO fair value adjustments. For the quarter, our non-interest expense to average asset ratio was 1.85% unchanged from last quarter, which reflects our continued success in expense management.\\n\\nNow moving on to Slide 8, I will discuss some of our key deposit trends. Our total deposits increased by approximately 2.4% from the end of the prior quarter with all of the growth coming in our lower cost categories. Our non-interest bearing deposits increased to 2.5% in the quarter, while money market and NOW deposits and savings account balances increased by 6.2% and 5.7% respectively.\\n\\nAs a result of the improvement in our deposit mix, long time deposits increased to 58.8% of our total deposits compared with 57.6% at the end of the prior quarter. We also continue to see positive trends in the repricing gap on time deposit renewals or the delta between a CD's expiring rate and the renewal rate. During the fourth quarter, renewed CDs were issued at our weighted average rate that was 59 basis point lower than the weighted average rate on those CDs that matured. As a result, the average cost of our time deposits declined 4 basis points from the preceding quarter.\\n\\nNow moving on to Slide 9. I will review our asset quality. Overall, our asset quality remains healthy and we continued to see positive trends. Our total criticized loans declined by another $7.7 million and now at our lowest levels since our MOE in 2016. However, our non-performing loans increased by $20.7 million from the end of the prior quarter. Approximately half of the increase was attributable to a $10 million construction loan that matured before the completion of the project. We chose not to extend the maturity date of the loan and stop funding new advances, which dictated that we placed the loan nonaccrual status. The project is above 85% complete and the current appraisal data shows lose that we are very well secured and no impairment exists. This is another example of our effort to proactively identify and manage potentially-problematic credits.\\n\\nWe are currently in a workout with a borrower with the expectation that the loan will be refinanced by another lender. The remainder of the increase in non-performing loans this quarter primarily relate to temporary delays in the renewal process, which also impacted the analysis of our 90-day plus delinquent loans on an accrual status.\\n\\nWe have worked through these delays for the most part already and we expect to see these loans migrate back to performing status by the end of the month. Our loss experience continues to be very low. We had just $738,000 of net charge-offs in the quarter or 2 basis points of average loans on an annualized basis.\\n\\nFor the full year, our net charge-offs amounted to adjust 4 basis points of average loans. Given the low level of net charge-offs and the lack of impairment in the new inflow into non-performing loans this quarter, we had a modest loan loss provision requirement of $1 million.\\n\\nWith that, let me turn the call back to Kevin.\\n\\nKevin S. Kim \\nThanks Alex. Let's move on to Slide 10. I will conclude with a few comments about our priorities and expectations for 2020. Similar to last year, we will be highly focused on deposit gathering and controlling expenses and we expect to see a continuation of the positive trends we have had in both areas. We anticipate that the environment for loan growth will continue to be challenging, as we expect that transaction activity in CRE market will remain sluggish. However, we expect to be more active in new loan production in 2020. We are also focusing on better defending our existing portfolio from the high level of payoffs we have seen in recent quarters. But importantly, our focus will continue to be on generating profitable growth. We will continue to be very disciplined in the loans we originate to ensure that we meet our targeted levels of profitability.\\n\\nOver the last year, we have made considerable investments in expanding our lending capacity beyond our traditional core markets. Our corporate banking group is focused on originating C&I loans and gathering corporate deposits from the middle market customer base. The experience of our expanded team of bankers has been diversified beyond financial services and now includes rather industry expertise in other areas.\\n\\nWe believe we have established a highly scalable business model and can continue to originate loans and gather deposits without the need to hire substantially more personnel. We are also optimistic about the growth opportunities in our residential mortgage and warehouse line business. We believe that we can add a number of new clients for our warehouse line business over the course of the year, which should result in higher funding volumes that will generate interest income. And in terms of our retail mortgage business, we are working to better leverage our entire branch network to generate more referrals from our existing customer base.\\n\\nTo date, we have had most of our residential mortgage originations coming from our mortgage loan production offices, which target the mainstream customer base. More recently, we have been focusing on growing our referrals that come through our Southern California branches and we are pleased to see growing origination volumes from these business units. We believe that with some additional focus and training in our other geographic markets, we can increase the referrals generated from our branches in other parts of the country, which will have the positive impact of generating higher levels of non-interest income from the gain on sale of other loans.\\n\\nCollectively, with the contributions from our corporate banking group and our warehouse line of credit business units, supplementing our traditional business lines, we believe we can generate a higher level of loan growth this year than we did in 2019. For 2020, we are budgeting for organic loan growth to range in the low-to-mid single-digits. And as always, we remain committed to strong capital management that includes strong shareholder returns, while maintaining strong capital ratios that support our continued growth. We have an attractive dividend that currently yields approximately 4% and we plan to remain active with our $50 million share repurchase program currently in place.\\n\\nTo wrap up, we ended 2019 on a strong note and we have a lot of momentum on our key initiatives as we start 2020. We believe we are well positioned to deliver another good year for our shareholders and further enhance the value of our franchise.\\n\\nWith that, let's open up the call to answer any questions you may have. Operator, please open up the call.\\n\\nQuestions And Answers\\nOperator\\nYes, thank you. We will now begin the question-and-answer session. (Operator Instructions) And the first question comes from Christopher McGratty with KBW.\\n\\nQ - Christopher McGratty \\nGreat, thanks, good afternoon. Kevin maybe you could start -- or Alex on the margin, obviously the environment has got a little bit tougher for the banks. But you have this -- the ability to bring down deposit costs, which you talked about in your prepared remarks. I appreciate in the guidance for the first quarter, but if the rate outlook remains steady, how do we think about ultimate stability in the flow and the margin, where and kind of when?\\n\\nA - Alex Ko \\nSure, sure. As I indicated, we would expect to have continued compression next quarter given the rate cuts that we have experienced especially October rate cut, it will continue next quarter. But as we indicated, our proactive deposit initiative as well as very disciplined pricing on the deposit, even though we have a very competitive -- competition on the loan rate is very still severe. We would expect to stabilize in the second quarter of 2020 in terms of net interest margin and then second half of the year, we would expect to start to increase.\\n\\nQ - Christopher McGratty \\nOkay, great. And as rates have come down maybe kind of a strategic question, Kevin, with the SBA business, any thoughts on potentially going back to the originate and sell model, maybe comment on where you thought, I think premiums might be today?\\n\\nA - Kevin S. Kim \\nOkay. Yeah. Because the average rate on SBA loans are considerably higher than the other loans, we have stopped selling the loans more than a year ago. And obviously, keeping this higher rate loans in our portfolio is an important element of our overall profitable growth strategy. So it is our intention to continue the retention policy for the time being, but as always, we have been very diligently monitoring the market situation and the current premiums in the secondary market is becoming better. So we will continue to evaluate, but at this time and for the time being, our intention is to continue to retain them on our books.\\n\\nQ - Christopher McGratty \\nOkay, great. Thank you.\\n\\nOperator\\nThank you. And the next question comes from Gary Tenner with DA Davidson.\\n\\nQ - Jake Stern \\nHi, good morning. This is actually Jake Stern on for Gary. How is it going today?\\n\\nA - Kevin S. Kim \\nGood.\\n\\nA - Angie Yang \\nFine.\\n\\nQ - Jake Stern \\nGood. So first question is the 10b5-1 trading program you announced in early December only runs through the end of January. Is it reasonable to expect you would utilize the full authorization by then or 1Q buybacks likely closer to 4Q levels?\\n\\nA - Kevin S. Kim \\nWell, as we noted in our earnings release, we were fairly active in the fourth quarter in buying back our own shares and we will continue to remain active in the first quarter. Whether we will be able to complete that $50 million program by the end of the first quarter will depend upon the overall market activity.\\n\\nQ - Jake Stern \\nOkay. And how are you thinking about buyback once the current authorization is fully used?\\n\\nA - Kevin S. Kim \\nWe do not have any additional plan in place at this time. But as in the past, we will continue to monitor our capital situation. And I would not be a surprised, depending upon the market situation and our capital situation, that the Board will consider another program later this year.\\n\\nQ - Jake Stern \\nOkay. Thanks for answering that.\\n\\nA - Angie Yang \\nJust to clarify Jake, February is not a blackout period. The 10b-51 program ends in January and our blackout -- which is when our blackout period ends.\\n\\nQ - Jake Stern \\nOkay. I appreciate you answering the question. Just another one to follow it up. What is the quarterly CD maturity schedule (inaudible) for 2020?\\n\\nA - Kevin S. Kim \\nSure. CD, we have, let's say, starting with the first quarter of 2020, total about $1.9 billion at an average rate of 2.17% is expected to mature. And the second quarter 2020, about a $1 billion at a rate of $2.33 billion. And Q3 is about $1.2 billion at a rate of 2.3% and the last quarter or Q4 of 2020, $960 million at a rate of 1.9%. So if I just aggregate all the 2020 maturity, that is about a 2.18% total of $5.2 billion.\\n\\nQ - Jake Stern \\nI appreciate the color on that. Just one more and then I'll step out here. So it looks like your expense to average assets has been at 1.85% level for three of the last four quarters. Are there opportunities to lower that level further or is that the level you are comfortable operating at?\\n\\nA - Kevin S. Kim \\nI feel comfortable operating it, but I would expect to have some improvement. And the reason for such improvement is we will continue to grow, but with a profitable growth. So as the average assets continue to grow and our very disciplined expense monitoring especially, there is some professional fees we are monitoring very closely and also no more need such as CECL, we did have a sizable amount of investment in 2019, but we do not expect that to happen or substantially decline in 2020. But there is some offsetting increases such as FDIC assessment fee and some other. So even assuming similar level of the total non-interest expenses, let's say $70 million or $71 million, I would expect to see the non-interest expense over average asset ratio slightly below than 1.85%.\\n\\nQ - Jake Stern \\nWonderful. I appreciate you guys answering my questions.\\n\\nA - Angie Yang \\nThank you.\\n\\nA - Kevin S. Kim \\nThank you.\\n\\nOperator\\nThank you. (Operator Instructions) And the next question comes from (inaudible) with Cap Security Management.\\n\\nQ - Unidentified Participant\\nGood afternoon everyone. Just a couple of questions. What percentage of your loans for the past quarter were fixed versus variable? And how do you see that trending during the next 12 months? And also, what do you -- are you anticipating from the loan portfolio going forward? Lower or higher from the current level of 5.04%?\\n\\nA - Kevin S. Kim \\nSure. Now let me start with the mix of the fixed and variable-rate loan. But as you know, we have mortgage loans so let me break into fixed, hybrid and variable. So hybrid what I mean is, it is hybrid, but if it is at a fixed rate, let's say first five years fixed and then variable, it's called hybrid, but if it is within the fixed, we call it as fixed. So total 6% at fixed rate is a 24% at a rate of about 4.6%. And hybrid is a 37% and the variable is 39%. And that 39% or $4.8 billion of variable rate loan consisted of mainly prime-based and also a LIBOR-based. So out of that entire variable rate loan, about 55% of loans are based on the prime and the remaining is based on the LIBOR and some other indexes, but the other index is very minor. So that's the kind of composition of the fixed and the variable and I think you did ask the second question about the loan pricing or the loan volume?\\n\\nA - Angie Yang \\nIt was yield expectation for loan.\\n\\nA - Kevin S. Kim \\nYield, so going forward, who knows how the interest rate rule [ph] be in the next 12 months or so, but the competition on the lending side is still very competitive. So I don't think in the (inaudible) increase on our loan yield, more realistically, we will see a small compression on our loan yields.\\n\\nQ - Unidentified Participant\\nAnd final question from me. You have -- what is your dividend policy going forward, because you haven't raised the dividend in six quarters. What would your criteria for potential dividend increases going forward?\\n\\nA - Kevin S. Kim \\nWe factor into a lot of considerations when we determine the dividend rate. And as I mentioned in the script, our current dividend yield is around 4% and which is pretty good. The comparative other -- compared with other players in the market and we also look at our overall capital position and dividend is one way of returning returns to our shareholders, but there will be other methods and currently, the $0.14 per quarter is the good level of dividend payout we believe. So unless there is a dramatic change in the market situation or our capital position, I think that will continue for the time being.\\n\\nA - Alex Ko \\nSo the dividend payout ratio is (inaudible) which we believe that's very healthy and also just to add, we have active share repurchase program and as Mr. Kim indicated, as needed we will continue to maximize our level of capital through either dividend or share repurchase and so forth.\\n\\nQ - Unidentified Participant\\nOkay, thank you very much, I appreciate it.\\n\\nA - Kevin S. Kim \\nThank you.\\n\\nOperator\\nAnd the next question comes from Tim Coffey with Janney.\\n\\nQ - Tim Coffey \\nThank you. Good morning everybody. Looking at the decline in gas station and car wash loans, down about $135 million, $136 million last year, what are some of the dynamics that are bringing those balances down?\\n\\nA - Kevin S. Kim \\nWe are looking at different marketplaces and depending on the industry and things we are trying to tighten up in certain areas we do see some risk. We have not made it any proactive efforts strategically to bring down some of those balances. But we are also just looking at the industry, we are also looking at the competition in terms of how aggressive they are getting. And I think you're seeing some of the competitiveness show up in terms of our balances there. But we don't see in terms of the overall market, we have concerns as we stated before in terms of where we are in the overall cycle. But in terms of the general markets and the industries, we don't see any big concerns in those balances. I think it's more of a reflection of the competition.\\n\\nQ - Tim Coffey \\nAnd if I may, the competition is I guess driving that market right now. Are those local banks or more national banks?\\n\\nA - Kevin S. Kim \\nWe're in a position where we see the competition from various places. So, we are seeing some of the large banks competing for our space, but we also have the smaller banks. So we do see a variety of all size banks.\\n\\nQ - Tim Coffey \\nOkay, great. Thank you, the rest of my questions have been answered.\\n\\nA - Angie Yang \\nThank you.\\n\\nA - Kevin S. Kim \\nThank you.\\n\\nA - Alex Ko \\nThank you.\\n\\nOperator\\nThank you. (Operator Instructions) And the next question comes from Matthew Clark with Piper Sandler.\\n\\nQ - Bob Sean \\nGood morning. This is actually Bob Sean on for Matthew. How are you guys doing?\\n\\nA - Angie Yang \\nFine.\\n\\nQ - Bob Sean \\nI just was wondering if you guys had a CECL update for us and any implementation guidance you can give?\\n\\nA - Kevin S. Kim\\nSure. We did kind of disclose in the past and we are in a good shape in terms of the progress because we are doing the second parallel run testing and there is not much changes from the previous -- our expectation. That's about the allowance for loan loss might increase on Day one due to the CECL implementation about 30% to 40% increase. It is subject to change and the capital impact from the CECL is kind of relatively minor. So I don't think there is any major changes from previous conversation and I do not expect major changes going forward as well.\\n\\nQ - Bob Sean\\nOkay, thanks. And I'd like to sneak one more in. Regarding the strength of your customers, have you seen positive trends in the cash flow ratios and loan to values of your customers or how has that trended over the past couple of quarters?\\n\\nA - Alex Ko\\nWe see in certain areas, a much stronger cash flow (Technical Difficulty) it's been a very good industry so far. And so, I think generally speaking, I think they've been pretty stable. I think we are seeing some industries that are showing improvements as hospitality as I mentioned before. But we have -- we are not seeing any like major changes I guess if that's the question you're asking, I think basically we're seeing positive or stable trends across industries.\\n\\nQ - Bob Sean\\nAll right, thanks. I'll step back.\\n\\nOperator\\nThank you. And that does conclude our question-and-answer session. So I would like to return the floor back over to management for any closing comments.\\n\\nA - Kevin S. Kim\\nOkay, thank you. Once again, everyone thank you for joining us today and we look forward to speaking with you again in three months. So long everyone.\\n\\nOperator\\nThank you. The conference has now concluded. Thank you for attending today's presentation. You may now disconnect your lines.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b) Print the first 400 characters of the text file you just loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿Company Name: Hope Bancorp Inc\n",
      "Company Ticker: HOPE US Equity\n",
      "Date: 2020-01-23\n",
      "Q4 2019 Earnings Call\n",
      "Company Participants\n",
      "Alex Ko, Executive Vice President and Chief Financial Officer\n",
      "Angie Yang, Director, Investor Relations\n",
      "Kevin S. Kim, President and Chief Executive Officer\n",
      "Other Participants\n",
      "Bob Sean, Analyst\n",
      "Christopher McGratty, Analyst\n",
      "Jake Stern, Analyst\n",
      "Tim Coffey, Analyst\n",
      "Unidentified Pa\n"
     ]
    }
   ],
   "source": [
    "print(transcript[:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c) Count the number of times the name `Angie` is mentioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript.count('Angie')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c) Use the provided Regular Expression to capture all numbers prior to a \"%\"  \n",
    "Use this regular expression: `\\W([\\.\\d]{,})%`  \n",
    "**You can play around with this regular expression here: <a href='https://bit.ly/3heIqoG'>Test on Pythex.org</a>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['31',\n",
       " '21',\n",
       " '2',\n",
       " '41.2',\n",
       " '48.3',\n",
       " '1.86',\n",
       " '1.88',\n",
       " '1.4',\n",
       " '5.7',\n",
       " '61',\n",
       " '31',\n",
       " '8',\n",
       " '3.16',\n",
       " '1.49',\n",
       " '1.54',\n",
       " '1.49',\n",
       " '1.44',\n",
       " '1.85',\n",
       " '2.4',\n",
       " '2.5',\n",
       " '6.2',\n",
       " '5.7',\n",
       " '58.8',\n",
       " '57.6',\n",
       " '85',\n",
       " '4',\n",
       " '2.17',\n",
       " '2.3',\n",
       " '1.9',\n",
       " '2.18',\n",
       " '1.85',\n",
       " '1.85',\n",
       " '5.04',\n",
       " '6',\n",
       " '24',\n",
       " '4.6',\n",
       " '37',\n",
       " '39',\n",
       " '39',\n",
       " '55',\n",
       " '4',\n",
       " '30',\n",
       " '40']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('\\W([\\.\\d]{,})%',transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\\W : extracts non-alphanumeric \n",
    ". extracts any character \n",
    "\\ escape special characters\n",
    "\\d digit\n",
    "{,} from m to n, m defaults to 0, n to infinity\n",
    "\n",
    "### It is capturing all numbers prior to %\n",
    "\n",
    "([\\.\\d]{,})%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1d) Load the text into a Spacy object and split it into a list of  sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to evaluate how well it worked by inspecting various elements of the sentence list.\n",
    "\n",
    "Note: the beginning of the document contains meta data that are not normal sentences, so you might see some weird \"sentences\" at the beginning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nlp will help to get rid of the unecessary characters at the beginning of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_t = nlp(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [x.text for x in nlp_t.sents]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sentences[150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the Hope Bancorp Q4 2019 Earnings Conference Call.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the Hope Bancorp Q4 2019 Earnings Conference Call.\n"
     ]
    }
   ],
   "source": [
    "print(sentences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1e) Parse out the following three parts of the earnings call transcript and put them in seperate variables:\n",
    "\n",
    "* The meta data at the top (e.g., company name, period, etc)   \n",
    "* The presentation portion  \n",
    "* The Q&A portion\n",
    "\n",
    "**Note:** you could do it based on the exact location (e.g, `text_file[:1234]`), however, that would only work for this file. Try to come up with a solution that would work for all files that follow the same structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pres_split = transcript.split('Presentation')\n",
    "meta = pres_split[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest = ''.join(pres_split[1:])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_split = rest.split('Questions And Answers')  \n",
    "presentation = qa_split[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA = ''.join(qa_split[1:]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1f) How many characters, sentences, words (tokens) do the presentation portion and the Q&A portion have?  \n",
    "\n",
    "Hint: use `Spacy` for the sentence and word counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##pres_nlp.sent is a spacy object and we can see the contents after we convert it into a list by the command list(pres_nlp.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## list(pres_nlp) gives a list of words in the nlp object pres_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pres_nlp = nlp(presentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The presentation has 17898 characters, 172 sentences and 3396 tokens.\n"
     ]
    }
   ],
   "source": [
    "print('The presentation has {} characters, {} sentences and {} tokens.'.format(len(presentation), \n",
    "                                                                               len(list(pres_nlp.sents)), \n",
    "                                                                               len(list(pres_nlp))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Create sentiment score based on Loughran and McDonald (2011)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a sentiment score for MD&As based on the Loughran and McDonald (2011) word lists.    \n",
    "\n",
    "#### References  \n",
    "\n",
    "*Loughran, T., & McDonald, B. (2011). When is a liability not a liability? Textual analysis, dictionaries, and 10‐Ks. The Journal of Finance, 66(1), 35-65.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data to use\n",
    "\n",
    "I have included a random selection of 20 pre-processed MDA filings in the `data > MDA_files` folder. The filename is the unique identifier.   \n",
    "\n",
    "You will also find a file called `MDA_META_DF.xlsx` in the \"data\" folder, this contains the following meta-data for each MD&A: \n",
    "* filing date  \n",
    "* cik   \n",
    "* company name  \n",
    "* link to filing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a) Load data into a dictionary with as key the filename and as value the content of the text file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files should all be in the following folder:  \n",
    "```\n",
    "os.path.join('data', 'MDA_files')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##os.listdir() is used to get the list of all files and directories in the specified directory. If we don’t specify any directory, then list of files and directories in the current working directory will be returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##os.path.join join one or more path components intelligently. Colon is the delimiter of the slice syntax to 'slice out' sub-parts in sequences [start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mda_file_folder = os.path.join('data', 'MDA_files')\n",
    "mda_files = [file for file in os.listdir(mda_file_folder) if file[-3:] == 'txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "mda_data = {}\n",
    "for mda_file in mda_files:\n",
    "    with open(os.path.join(mda_file_folder, mda_file), 'r') as f:\n",
    "     mda_data[mda_file] = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b) Load the Loughran and McDonald master dictionary    \n",
    "**Note:** The Loughran and McDonald dictionary is included in the \"data\" folder: `LoughranMcDonald_MasterDictionary_2014.xlsx `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_df = pd.read_excel(os.path.join('data', 'LoughranMcDonald_MasterDictionary_2014.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Sequence Number</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Word Proportion</th>\n",
       "      <th>Average Proportion</th>\n",
       "      <th>Std Dev</th>\n",
       "      <th>Doc Count</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Uncertainty</th>\n",
       "      <th>Litigious</th>\n",
       "      <th>Constraining</th>\n",
       "      <th>Superfluous</th>\n",
       "      <th>Interesting</th>\n",
       "      <th>Modal</th>\n",
       "      <th>Irr_Verb</th>\n",
       "      <th>Harvard_IV</th>\n",
       "      <th>Syllables</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AARDVARK</td>\n",
       "      <td>1</td>\n",
       "      <td>81</td>\n",
       "      <td>5.690194e-09</td>\n",
       "      <td>3.068740e-09</td>\n",
       "      <td>5.779943e-07</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AARDVARKS</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.404986e-10</td>\n",
       "      <td>8.217606e-12</td>\n",
       "      <td>7.841870e-09</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABACI</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>5.619945e-10</td>\n",
       "      <td>1.686149e-10</td>\n",
       "      <td>7.096240e-08</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABACK</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3.512466e-10</td>\n",
       "      <td>1.727985e-10</td>\n",
       "      <td>7.532677e-08</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABACUS</td>\n",
       "      <td>5</td>\n",
       "      <td>1752</td>\n",
       "      <td>1.230768e-07</td>\n",
       "      <td>1.198634e-07</td>\n",
       "      <td>1.110293e-05</td>\n",
       "      <td>465</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85126</th>\n",
       "      <td>ZYGOTE</td>\n",
       "      <td>85127</td>\n",
       "      <td>35</td>\n",
       "      <td>2.458726e-09</td>\n",
       "      <td>1.025127e-09</td>\n",
       "      <td>2.320929e-07</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85127</th>\n",
       "      <td>ZYGOTES</td>\n",
       "      <td>85128</td>\n",
       "      <td>1</td>\n",
       "      <td>7.024931e-11</td>\n",
       "      <td>2.593031e-11</td>\n",
       "      <td>2.474469e-08</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85128</th>\n",
       "      <td>ZYGOTIC</td>\n",
       "      <td>85129</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85129</th>\n",
       "      <td>ZYMURGIES</td>\n",
       "      <td>85130</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85130</th>\n",
       "      <td>ZYMURGY</td>\n",
       "      <td>85131</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>85131 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Word  Sequence Number  Word Count  Word Proportion  \\\n",
       "0       AARDVARK                1          81     5.690194e-09   \n",
       "1      AARDVARKS                2           2     1.404986e-10   \n",
       "2          ABACI                3           8     5.619945e-10   \n",
       "3          ABACK                4           5     3.512466e-10   \n",
       "4         ABACUS                5        1752     1.230768e-07   \n",
       "...          ...              ...         ...              ...   \n",
       "85126     ZYGOTE            85127          35     2.458726e-09   \n",
       "85127    ZYGOTES            85128           1     7.024931e-11   \n",
       "85128    ZYGOTIC            85129           0     0.000000e+00   \n",
       "85129  ZYMURGIES            85130           0     0.000000e+00   \n",
       "85130    ZYMURGY            85131           0     0.000000e+00   \n",
       "\n",
       "       Average Proportion       Std Dev  Doc Count  Negative  Positive  \\\n",
       "0            3.068740e-09  5.779943e-07         45         0         0   \n",
       "1            8.217606e-12  7.841870e-09          1         0         0   \n",
       "2            1.686149e-10  7.096240e-08          7         0         0   \n",
       "3            1.727985e-10  7.532677e-08          5         0         0   \n",
       "4            1.198634e-07  1.110293e-05        465         0         0   \n",
       "...                   ...           ...        ...       ...       ...   \n",
       "85126        1.025127e-09  2.320929e-07         25         0         0   \n",
       "85127        2.593031e-11  2.474469e-08          1         0         0   \n",
       "85128        0.000000e+00  0.000000e+00          0         0         0   \n",
       "85129        0.000000e+00  0.000000e+00          0         0         0   \n",
       "85130        0.000000e+00  0.000000e+00          0         0         0   \n",
       "\n",
       "       Uncertainty  Litigious  Constraining  Superfluous  Interesting  Modal  \\\n",
       "0                0          0             0            0            0      0   \n",
       "1                0          0             0            0            0      0   \n",
       "2                0          0             0            0            0      0   \n",
       "3                0          0             0            0            0      0   \n",
       "4                0          0             0            0            0      0   \n",
       "...            ...        ...           ...          ...          ...    ...   \n",
       "85126            0          0             0            0            0      0   \n",
       "85127            0          0             0            0            0      0   \n",
       "85128            0          0             0            0            0      0   \n",
       "85129            0          0             0            0            0      0   \n",
       "85130            0          0             0            0            0      0   \n",
       "\n",
       "       Irr_Verb  Harvard_IV  Syllables     Source  \n",
       "0             0           0          2  12of12inf  \n",
       "1             0           0          2  12of12inf  \n",
       "2             0           0          3  12of12inf  \n",
       "3             0           0          2  12of12inf  \n",
       "4             0           0          3  12of12inf  \n",
       "...         ...         ...        ...        ...  \n",
       "85126         0           0          2  12of12inf  \n",
       "85127         0           0          2  12of12inf  \n",
       "85128         0           0          3  12of12inf  \n",
       "85129         0           0          3  12of12inf  \n",
       "85130         0           0          3  12of12inf  \n",
       "\n",
       "[85131 rows x 19 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c) Create two lists: one containing all the negative words and the other one containing all the positive words   \n",
    "\n",
    "Note, you can treat any number that is not 0 as a 1:\n",
    "\n",
    "```python\n",
    "0         ## <-- 0\n",
    "2009      ## <-- 1\n",
    "2014      ## <-- 1\n",
    "2011      ## <-- 1\n",
    "2012      ## <-- 1\n",
    "```\n",
    "\n",
    "They include the year instead of a one for versioning purposes. \n",
    "\n",
    "**Tip:** I recommend to change all words to lowercase in this step so that you don't need to worry about that later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_words = list(lm_df[lm_df.Negative != 0].Word.astype(str).str.lower().values)\n",
    "pos_words = list(lm_df[lm_df.Positive != 0].Word.astype(str).str.lower().values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .str. is an accessor that is used to access different functions such as lower() and upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2d) For each MD&A calculate the *total* number of times negative and positive words are mentioned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Note:** make sure you deal with uppercase vs. lowercase and substring matches.\n",
    "\n",
    "**Hint 1:** save the counts to a list where each entry is a list that contains the following three items: [*filename*, *total pos count*, *total neg count*], like this:\n",
    "> [   \n",
    "    ['21344_0000021344-16-000050.txt', 1234, 1234],   \n",
    "    ['21510_0000021510-16-000074.txt', 1234, 1234],  \n",
    "> ....  \n",
    " ]   \n",
    " \n",
    "An example to illustrate sub-string matches:\n",
    "\n",
    "```python\n",
    "### For example, consider the positive word 'win'\n",
    "\n",
    "test_sen = \"They hockey team made a big win during the winter.\"\n",
    "\n",
    "test_sen.count('win')\n",
    "## gives --> 2 \n",
    "\n",
    "## We only want to count \"win\" not \"winter\", how do we solve that?\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "mda_term_counts = []\n",
    "for mda_file, mda_text in mda_data.items():\n",
    "    text_lower = mda_text.lower()\n",
    "    \n",
    "    pos_count = 0\n",
    "    for word in pos_words:\n",
    "        pos_count += text_lower.count('' + word + '')\n",
    "        \n",
    "    neg_count = 0\n",
    "    for word in neg_words:\n",
    "        neg_count += text_lower.count('' + word + '')   \n",
    "        \n",
    "    mda_term_counts.append([mda_file,pos_count,neg_count])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['36146_0001564590-16-013066.txt', 684, 2094]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mda_term_counts[15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2e) Convert the list created in 3c into a Pandas DataFrame  \n",
    "**Hint:** Use the `columns=[...]` parameter to name the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data = mda_term_counts, columns = ['file_name', 'total pos count', 'total neg count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>total pos count</th>\n",
       "      <th>total neg count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26076_0001558370-16-010242.txt</td>\n",
       "      <td>470</td>\n",
       "      <td>1214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30625_0000030625-16-000121.txt</td>\n",
       "      <td>400</td>\n",
       "      <td>1268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26324_0000026324-16-000040.txt</td>\n",
       "      <td>378</td>\n",
       "      <td>666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40533_0000040533-16-000056.txt</td>\n",
       "      <td>328</td>\n",
       "      <td>494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46250_0000046250-16-000047.txt</td>\n",
       "      <td>200</td>\n",
       "      <td>458</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        file_name  total pos count  total neg count\n",
       "0  26076_0001558370-16-010242.txt              470             1214\n",
       "1  30625_0000030625-16-000121.txt              400             1268\n",
       "2  26324_0000026324-16-000040.txt              378              666\n",
       "3  40533_0000040533-16-000056.txt              328              494\n",
       "4  46250_0000046250-16-000047.txt              200              458"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2f) Create a new column with a \"sentiment score\" for each MD&A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following imaginary sentiment score:  \n",
    "$$\\frac{(Num\\ Positive\\ Words - Num\\ Negative\\ Words)}{Sum\\ of Pos\\ and\\ Neg\\ Words}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = (df['total pos count'] - df['total neg count'])/(df['total pos count'] + df['total neg count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>total pos count</th>\n",
       "      <th>total neg count</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26076_0001558370-16-010242.txt</td>\n",
       "      <td>470</td>\n",
       "      <td>1214</td>\n",
       "      <td>-0.441805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30625_0000030625-16-000121.txt</td>\n",
       "      <td>400</td>\n",
       "      <td>1268</td>\n",
       "      <td>-0.520384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26324_0000026324-16-000040.txt</td>\n",
       "      <td>378</td>\n",
       "      <td>666</td>\n",
       "      <td>-0.275862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40533_0000040533-16-000056.txt</td>\n",
       "      <td>328</td>\n",
       "      <td>494</td>\n",
       "      <td>-0.201946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46250_0000046250-16-000047.txt</td>\n",
       "      <td>200</td>\n",
       "      <td>458</td>\n",
       "      <td>-0.392097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>23217_0001628280-16-017613.txt</td>\n",
       "      <td>450</td>\n",
       "      <td>1470</td>\n",
       "      <td>-0.531250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21510_0000021510-16-000074.txt</td>\n",
       "      <td>606</td>\n",
       "      <td>1078</td>\n",
       "      <td>-0.280285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>47518_0001214659-16-014806.txt</td>\n",
       "      <td>482</td>\n",
       "      <td>974</td>\n",
       "      <td>-0.337912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>49071_0000049071-16-000117.txt</td>\n",
       "      <td>954</td>\n",
       "      <td>1636</td>\n",
       "      <td>-0.263320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>47217_0000047217-16-000093.txt</td>\n",
       "      <td>544</td>\n",
       "      <td>928</td>\n",
       "      <td>-0.260870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>36047_0000036047-16-000065.txt</td>\n",
       "      <td>258</td>\n",
       "      <td>960</td>\n",
       "      <td>-0.576355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>26780_0000026780-16-000019.txt</td>\n",
       "      <td>898</td>\n",
       "      <td>1152</td>\n",
       "      <td>-0.123902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>21665_0001628280-16-011343.txt</td>\n",
       "      <td>516</td>\n",
       "      <td>1058</td>\n",
       "      <td>-0.344346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>43920_0000043920-16-000025.txt</td>\n",
       "      <td>558</td>\n",
       "      <td>1568</td>\n",
       "      <td>-0.475071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>26058_0000026058-16-000044.txt</td>\n",
       "      <td>236</td>\n",
       "      <td>408</td>\n",
       "      <td>-0.267081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>36146_0001564590-16-013066.txt</td>\n",
       "      <td>684</td>\n",
       "      <td>2094</td>\n",
       "      <td>-0.507559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>21344_0000021344-16-000050.txt</td>\n",
       "      <td>1166</td>\n",
       "      <td>2318</td>\n",
       "      <td>-0.330654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>31462_0001558370-16-003581.txt</td>\n",
       "      <td>726</td>\n",
       "      <td>1092</td>\n",
       "      <td>-0.201320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>34903_0000034903-16-000045.txt</td>\n",
       "      <td>290</td>\n",
       "      <td>818</td>\n",
       "      <td>-0.476534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>35214_0001562762-16-000341.txt</td>\n",
       "      <td>496</td>\n",
       "      <td>1188</td>\n",
       "      <td>-0.410926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         file_name  total pos count  total neg count  \\\n",
       "0   26076_0001558370-16-010242.txt              470             1214   \n",
       "1   30625_0000030625-16-000121.txt              400             1268   \n",
       "2   26324_0000026324-16-000040.txt              378              666   \n",
       "3   40533_0000040533-16-000056.txt              328              494   \n",
       "4   46250_0000046250-16-000047.txt              200              458   \n",
       "5   23217_0001628280-16-017613.txt              450             1470   \n",
       "6   21510_0000021510-16-000074.txt              606             1078   \n",
       "7   47518_0001214659-16-014806.txt              482              974   \n",
       "8   49071_0000049071-16-000117.txt              954             1636   \n",
       "9   47217_0000047217-16-000093.txt              544              928   \n",
       "10  36047_0000036047-16-000065.txt              258              960   \n",
       "11  26780_0000026780-16-000019.txt              898             1152   \n",
       "12  21665_0001628280-16-011343.txt              516             1058   \n",
       "13  43920_0000043920-16-000025.txt              558             1568   \n",
       "14  26058_0000026058-16-000044.txt              236              408   \n",
       "15  36146_0001564590-16-013066.txt              684             2094   \n",
       "16  21344_0000021344-16-000050.txt             1166             2318   \n",
       "17  31462_0001558370-16-003581.txt              726             1092   \n",
       "18  34903_0000034903-16-000045.txt              290              818   \n",
       "19  35214_0001562762-16-000341.txt              496             1188   \n",
       "\n",
       "    sentiment  \n",
       "0   -0.441805  \n",
       "1   -0.520384  \n",
       "2   -0.275862  \n",
       "3   -0.201946  \n",
       "4   -0.392097  \n",
       "5   -0.531250  \n",
       "6   -0.280285  \n",
       "7   -0.337912  \n",
       "8   -0.263320  \n",
       "9   -0.260870  \n",
       "10  -0.576355  \n",
       "11  -0.123902  \n",
       "12  -0.344346  \n",
       "13  -0.475071  \n",
       "14  -0.267081  \n",
       "15  -0.507559  \n",
       "16  -0.330654  \n",
       "17  -0.201320  \n",
       "18  -0.476534  \n",
       "19  -0.410926  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2g) Use the `MDA_META_DF` file to add the company name, filing date, and CIK to the sentiment dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "mda_meta_df = pd.read_excel(os.path.join('data', 'MDA_META_DF.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>total pos count</th>\n",
       "      <th>total neg count</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>fdate</th>\n",
       "      <th>cik</th>\n",
       "      <th>coname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26076_0001558370-16-010242.txt</td>\n",
       "      <td>470</td>\n",
       "      <td>1214</td>\n",
       "      <td>-0.441805</td>\n",
       "      <td>2016-11-22</td>\n",
       "      <td>26076</td>\n",
       "      <td>CUBIC CORP /DE/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30625_0000030625-16-000121.txt</td>\n",
       "      <td>400</td>\n",
       "      <td>1268</td>\n",
       "      <td>-0.520384</td>\n",
       "      <td>2016-02-18</td>\n",
       "      <td>30625</td>\n",
       "      <td>FLOWSERVE CORP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26324_0000026324-16-000040.txt</td>\n",
       "      <td>378</td>\n",
       "      <td>666</td>\n",
       "      <td>-0.275862</td>\n",
       "      <td>2016-02-25</td>\n",
       "      <td>26324</td>\n",
       "      <td>CURTISS WRIGHT CORP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40533_0000040533-16-000056.txt</td>\n",
       "      <td>328</td>\n",
       "      <td>494</td>\n",
       "      <td>-0.201946</td>\n",
       "      <td>2016-02-08</td>\n",
       "      <td>40533</td>\n",
       "      <td>GENERAL DYNAMICS CORP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46250_0000046250-16-000047.txt</td>\n",
       "      <td>200</td>\n",
       "      <td>458</td>\n",
       "      <td>-0.392097</td>\n",
       "      <td>2016-06-03</td>\n",
       "      <td>46250</td>\n",
       "      <td>HAWKINS INC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        file_name  total pos count  total neg count  \\\n",
       "0  26076_0001558370-16-010242.txt              470             1214   \n",
       "1  30625_0000030625-16-000121.txt              400             1268   \n",
       "2  26324_0000026324-16-000040.txt              378              666   \n",
       "3  40533_0000040533-16-000056.txt              328              494   \n",
       "4  46250_0000046250-16-000047.txt              200              458   \n",
       "\n",
       "   sentiment      fdate    cik                 coname  \n",
       "0  -0.441805 2016-11-22  26076        CUBIC CORP /DE/  \n",
       "1  -0.520384 2016-02-18  30625         FLOWSERVE CORP  \n",
       "2  -0.275862 2016-02-25  26324    CURTISS WRIGHT CORP  \n",
       "3  -0.201946 2016-02-08  40533  GENERAL DYNAMICS CORP  \n",
       "4  -0.392097 2016-06-03  46250            HAWKINS INC  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = pd.merge(df,mda_meta_df[['file_name','fdate','cik','coname']], how = 'left', on = 'file_name') \n",
    "new_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3a) Calculate the Term Frequency (TF) vectors for the MD&A files. \n",
    "\n",
    "You should end up with a matrix of the shape 20x6747 (or something along those lines). 20 reflects the number of MDA filings and 6747 reflects the number of unique tokens/words.\n",
    "\n",
    "**Use the `CountVectorizer` from sci-kit learn:** https://scikit-learn.org/stable/modules/feature_extraction.html#common-vectorizer-usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26076_0001558370-16-010242.txt</th>\n",
       "      <td>We are a leading international provider of cos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30625_0000030625-16-000121.txt</th>\n",
       "      <td>The following discussion and analysis is provi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26324_0000026324-16-000040.txt</th>\n",
       "      <td>Curtiss-Wright Corporation and its subsidiarie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40533_0000040533-16-000056.txt</th>\n",
       "      <td>For an overview of our business groups, includ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46250_0000046250-16-000047.txt</th>\n",
       "      <td>The following is a discussion and analysis of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23217_0001628280-16-017613.txt</th>\n",
       "      <td>The following discussion and analysis is inten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21510_0000021510-16-000074.txt</th>\n",
       "      <td>Below is a summary of some of the quantitative...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47518_0001214659-16-014806.txt</th>\n",
       "      <td>We are a leading global medical technology com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49071_0000049071-16-000117.txt</th>\n",
       "      <td>Humana Inc., headquartered in Louisville, Kent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47217_0000047217-16-000093.txt</th>\n",
       "      <td>An analysis of our continuing financial result...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36047_0000036047-16-000065.txt</th>\n",
       "      <td>This Annual Report on Form 10-K and certain in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26780_0000026780-16-000019.txt</th>\n",
       "      <td>We are a global provider of high technology dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21665_0001628280-16-011343.txt</th>\n",
       "      <td>To this end, the Company is tightly focused on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43920_0000043920-16-000025.txt</th>\n",
       "      <td>We developed and use the Greif Business System...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26058_0000026058-16-000044.txt</th>\n",
       "      <td>CTS' vision is to be a leading provider of sen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36146_0001564590-16-013066.txt</th>\n",
       "      <td>The following provides a narrative discussion ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21344_0000021344-16-000050.txt</th>\n",
       "      <td>The following Management's Discussion and Anal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31462_0001558370-16-003581.txt</th>\n",
       "      <td>The following management discussion and analys...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34903_0000034903-16-000045.txt</th>\n",
       "      <td>The following discussion should be read in con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35214_0001562762-16-000341.txt</th>\n",
       "      <td>During the year ended December 31, 2015, net s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                          content\n",
       "26076_0001558370-16-010242.txt  We are a leading international provider of cos...\n",
       "30625_0000030625-16-000121.txt  The following discussion and analysis is provi...\n",
       "26324_0000026324-16-000040.txt  Curtiss-Wright Corporation and its subsidiarie...\n",
       "40533_0000040533-16-000056.txt  For an overview of our business groups, includ...\n",
       "46250_0000046250-16-000047.txt  The following is a discussion and analysis of ...\n",
       "23217_0001628280-16-017613.txt  The following discussion and analysis is inten...\n",
       "21510_0000021510-16-000074.txt  Below is a summary of some of the quantitative...\n",
       "47518_0001214659-16-014806.txt  We are a leading global medical technology com...\n",
       "49071_0000049071-16-000117.txt  Humana Inc., headquartered in Louisville, Kent...\n",
       "47217_0000047217-16-000093.txt  An analysis of our continuing financial result...\n",
       "36047_0000036047-16-000065.txt  This Annual Report on Form 10-K and certain in...\n",
       "26780_0000026780-16-000019.txt  We are a global provider of high technology dr...\n",
       "21665_0001628280-16-011343.txt  To this end, the Company is tightly focused on...\n",
       "43920_0000043920-16-000025.txt  We developed and use the Greif Business System...\n",
       "26058_0000026058-16-000044.txt  CTS' vision is to be a leading provider of sen...\n",
       "36146_0001564590-16-013066.txt  The following provides a narrative discussion ...\n",
       "21344_0000021344-16-000050.txt  The following Management's Discussion and Anal...\n",
       "31462_0001558370-16-003581.txt  The following management discussion and analys...\n",
       "34903_0000034903-16-000045.txt  The following discussion should be read in con...\n",
       "35214_0001562762-16-000341.txt  During the year ended December 31, 2015, net s..."
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf = pd.DataFrame.from_dict(mda_data, orient = 'index', columns = ['content'])\n",
    "newdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a vectorizer function using CountVectorizer to convert the text into term frequencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer(stop_words = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectors = tf_vectorizer.fit_transform(newdf.content.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This function converts the text values into a matrix of term frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse._csr.csr_matrix"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tf_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This reflects the shape of the scipy matrix with 2 files and 6519 unique tokens or words for which we have frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 6519)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Set max_df and min_df values in the CountVectorizer function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###max_df = 0.8 --> any word that occurs in more than 80% of our documents is not going to be included; min_df is the minimum number of documents in which the word should occur. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer(stop_words = 'english', max_df = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "##tf_vectorizer = CountVectorizer(stop_words = 'english', max_df = 0.9, min_df = 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = list(tf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tf_vectors.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>6509</th>\n",
       "      <th>6510</th>\n",
       "      <th>6511</th>\n",
       "      <th>6512</th>\n",
       "      <th>6513</th>\n",
       "      <th>6514</th>\n",
       "      <th>6515</th>\n",
       "      <th>6516</th>\n",
       "      <th>6517</th>\n",
       "      <th>6518</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 6519 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0     1     2     3     4     5     6     7     8     9     ...  6509  \\\n",
       "0      0     0     0     0     0     0     2     0     0     2  ...     2   \n",
       "1      2     3     1     0     0     0     0     0     0     0  ...     0   \n",
       "2      2     6     0     0     0     0     0     0     0     0  ...     2   \n",
       "3      0     0     0     0     0     0     8     0     0    10  ...     8   \n",
       "4      2     2     0     0     0     0     0     0     0     0  ...     0   \n",
       "5      0     0     0     0     0     0     0     0     0     0  ...     6   \n",
       "6      6     0     0     0     0     0     0     0     0     2  ...     2   \n",
       "7      0     2     0     0     0     0     0     0     0     0  ...     2   \n",
       "8      0    34     0     0     0     0     0     0     0     0  ...     0   \n",
       "9      0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "10     4     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "11     2    10     0     0     0     0     0     0     0     0  ...     8   \n",
       "12     6     2     0     0     0     0     2     2     0     4  ...     6   \n",
       "13     4     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "14     0    10     0     0     0     0     0     0     2     0  ...     0   \n",
       "15     6     0     0     0     0     0     2     4     0     6  ...    42   \n",
       "16     0     2     0     1     1     0     0     0     0     0  ...    10   \n",
       "17     6     0     0     0     0     0     0     2     0     0  ...    10   \n",
       "18     0    26     0     0     0     2     2     0     0     0  ...     2   \n",
       "19     0     0     0     0     0     0     0     0     0     0  ...     6   \n",
       "\n",
       "    6510  6511  6512  6513  6514  6515  6516  6517  6518  \n",
       "0      0     0     2     6     0    10     4     0     0  \n",
       "1      4     0     0     0     0     0     0     0     0  \n",
       "2      0     0     0     0     0     0     0     0     0  \n",
       "3      0     0     0     0     0     0     0     0     0  \n",
       "4      0     0     0     2     0     0     0     0     0  \n",
       "5      0     0     0     0     0     0     0     0     0  \n",
       "6      0     0     4     0     0     0     0     0     0  \n",
       "7      0     0     0     0     0     0     0     0     0  \n",
       "8      0     0     4     0     0     0     0     0     0  \n",
       "9      0     0     2     2     0     0     0     0     0  \n",
       "10     0     0     0     0     0     0     0     2     0  \n",
       "11     0     0     2     0     0     0     0     2     0  \n",
       "12     0     0     0     0     0     0     0     0     0  \n",
       "13     0     0     4     2     0     0     0     0     0  \n",
       "14     0     0     0     0     0     0     0     0     0  \n",
       "15     0     2     6     0     0     0     4     0     0  \n",
       "16     0     0     0     0     0     0     8     0     0  \n",
       "17     0     2     0     0     0     0     0     0     0  \n",
       "18     0     0     0     0     0     0     0     0     4  \n",
       "19     0     0     2     0     2     0     2     0     0  \n",
       "\n",
       "[20 rows x 6519 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tf_vectors.toarray())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
